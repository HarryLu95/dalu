---
title: "Lab 3 - On the Effect of Centering Variables"
output:
  pdf_document: default
  html_document:
    highlight: pygments
    theme: cerulean
  word_document: default
---
-----

# Lab Goals

The first two questions in this lab are designed to make you practice some theoretical calculations, while the third confirms these results using the \texttt{hills} data from Lab 2.  When going through these, you may find it helps to have a very simple example to work through, either by hand or in R. Here we recommend using a $p =2$ case with
$$
\boldmath{x}_1 = (1,2,2,3), \ \boldmath{x}_2 = (1,1,2,4), \ \beta = (1,1,1), \ \boldmath{y} = (2,4,6,8)
$$

We'll set this up in R:
```{r}
x1 = c(1,2,2,3)
x2 = c(1,1,2,4)
beta = c(1,1,1)
y = c(2,4,6,8)
```
and we'll also define $X$ and $C$ matrices
```{r}
X = cbind( rep(1,4), x1, x2)
C = diag(4) - matrix(1/4,4,4)
```
which we will use later.  It will also help to define centered versions of covariates:
```{r}
x1c = C%*%x1
x2c = C%*%x2
Xc = cbind( rep(1,4), x1c, x2c)
```

# Centering, Linear Regression and ANOVAs

In class, we've relied on centered variables to clean up some calculations, and we've (rather blythely) said that this is OK (see the derivation of VIFs for example). Here we'll work through showing that this is the case:

1. By manipulating the equation
$$
\boldmath{y} = \beta_0 + \beta_1 \boldmath{x}_1 + \ldots + \beta_p \boldmath{x}_p + \epsilon
$$
show that changing covariates from $\boldmath{x}_j$ to $\boldmath{x}^*_j = C\boldmath{x}_j$ changes $\beta_0$ but not $\beta_j$.

*Solution*
\begin{align*}
\boldmath{y} & = \beta_0 + \beta_1 \bar{\boldmath{x}}_1 + \ldots + \bar{\beta_p} \boldmath{x}_p +  \beta_1 (\boldmath{x}_1 - \bar{\boldmath{x}}_1) + \ldots + \beta_p (\boldmath{x}_p - \bar{\boldmath{x}}_p) + \epsilon \\
& = \beta_0^* + \beta_1 \boldmath{x}_1 + \ldots + \beta_p \boldmath{x}_p + \epsilon
\end{align*}



2. Show that the *estimated* $\hat{\beta}_j$ are also unaffected by centering the covariates. Do this by arguing that the *fitted* *values* are the same and these minimize mean squared error. Below, we will show that this is true by explicitly using matrix algebra. 


*Solution*

The fitted values can be expressed as 

\begin{align*}
\hat{\boldmath{y}} & = \hat{\beta_0} + \hat{\beta_1} \bar{\boldmath{x}}_1 + \ldots + \hat{\bar{\beta_p}} \boldmath{x}_p +  \hat{\beta_1} (\boldmath{x}_1 - \bar{\boldmath{x}}_1) + \ldots + \hat{\beta}_p (\boldmath{x}_p - \bar{\boldmath{x}}_p) + \epsilon \\
& = \hat{\beta}_0^* + \hat{\beta_1} \boldmath{x}_1 + \ldots + \hat{\beta_p} \boldmath{x}_p + \epsilon
\end{align*}


In R, we see that we can run either
```{r,echo=TRUE}
mod = lm(y~x1+x2)
mod$coef
```
or
```{r,echo=TRUE}
modc = lm(y~x1c+x2c)
modc$coef
```

3. In simple linear regression, when $\boldmath{x}$ is centered, there is a simple formula for $\hat{\beta}_0$ and $\hat{\beta}_1$ and these are uncorrelated.  When $\boldmath{x}$ is *not* centered, use the formula that for a $2 \times 2$ matrix,
$$
    \left[ \begin{array}{cc} a & b \\ c & d \end{array} \right]^{-1} = \frac{1}{ad-cb} \left[ \begin{array}{cc} d & -c \\ -b & a \end{array} \right]
$$
    to find the covariance of $(\hat{\beta}_0,\hat{\beta}_1)$ and show that centering reduces the variance of $\hat{\beta}_0$ but leaves the variance of $\hat{\beta}_1$ unchanged. \label{invariant}

*Solution*
Here for $X^TX$ we have $a = n$, $b = c= n \bar{x}$ and $d = x^Tx$ and the formula gives us that
\[
(X^TX)^{-1} = \frac{1}{n x^T x - n^2 \bar{x}^2}\left[ \begin{array}{cc} x^T X & - n\bar{x} \\ -n \bar{x} & n \end{array}\right] = \left[ \begin{array}{cc} (SXX + n \bar{x}^2)/nSXX & -\bar{x}/SXX \\ -\bar{x}/SXX & 1/SXX \end{array}\right]
\]
because $SXX = x^T x - n \bar{x}^2$. 

So cov$(\hat{beta}_0, \hat{\beta}_1) = - \bar{x}/SXX$ and we observe that if $\bar{x} = 0$ the 2,2 entry of this matrix does not change, but the 1,1 entry is reduced by $\bar{x}^2/SXX$. 



In our toy example in R, we can consider only the first covariate and look at
```{r,echo=TRUE}
mod1 = lm(y~x1)
vcov(mod1)
```
versus
```{r,echo=TRUE}
mod1c = lm(y~x1c)
vcov(mod1c)
```

4. If $X$ is the design matrix, including the intercept, for the model above, find $A$ so that $XA = X^*$ where the columns of $X^*$ are all centered except for the first column of 1's.


*Solution*
\[
A = \left[ \begin{array}{cc} 1 & -\bar{\boldmath{x}}^T \\ \boldmath{0} & I \end{array} \right]
\]
where $\bar{\boldmath{x}}^T = (\bar{x}_1,\bar{x}_2,\ldots,\bar{x}_p)$ and $\boldmath{0}$ is a vector of $p$ 0's. 


As an example in R, if we consider
```{r}
A = matrix( c(1,-2,-2,0,1,0,0,0,1),3,3,byrow=TRUE)
X%*%A
```
compare to 
```{r}
Xc
```

5. Find $A^{-1}$ from the previous question (hint: $A$ will be given in terms of the $\bar{x}_j$, replace each of these with $- \bar{x}_j$ and show that this gives you the inverse).

*Solution*
\[
\left[ \begin{array}{cc} 1 & -\bar{\boldmath{x}}^T \\ \boldmath{0} & I \end{array} \right]\left[ \begin{array}{cc} 1 & \bar{\boldmath{x}}^T \\ \boldmath{0} & I \end{array} \right] = \left[ \begin{array}{cc} 1 &  \bar{\boldmath{x}}^T  -\bar{\boldmath{x}}^T \\ \boldmath{0} & I \end{array} \right]  = I_{p+1}
\]



```{r}
solve(A)
```

5. By expanding $(X^{*T} X^*)^{-1}$ in terms of $(X^T X)^{-1}$ and $A^{-1}$, show that the result from \eqref{invariant} is also true of multiple regression. Hence our Variance Inflation Factor identity also holds for un-centered covariates.

*Solution*
I will write $S = (X^TX){_1}$ for convenientce, then
\[
A^{-1} (X^TX)^{-1} (A^{-1})^T  = \left[ \begin{array}{cc} S_{11} + S_{1,-1} \bar{\boldmath{x}} + \bar{\boldmath{x}}^T S_{-1,1} + \bar{\boldmath{x}}^T S_{-1,-1} \bar{\boldmath{x}} & S_{1,-1} + \bar{\boldmath{x}}^T S_{-1,-1} \\ S_{-1,1} + S_{-1,-1} \bar{\boldmath{x}}  & S_{-1,-1}  \end{array} \right]
\]
where we seee that only the first row and column are changed, and that the (1,1)th entry is smallest when $\bar{\boldmath{x}} = 0$. 



```{r}
vcov(mod)
vcov(modc)
```

# In Data

Let's look at what we actually get in data. For this question, we use the data used in Lab 2.


```{r}
setwd("C:/Users/lib-pac-b30a/Downloads")
hills=read.csv("hills.csv")
fit=lm(Time~Distance+Climb,data=hills)
summary(fit)
```

1. Create a centering matrix and apply this to center Distance and Climb. Show that applying lm to these data gives you the same slopes, but changes the intercept.
```{r,echo=TRUE}
# Centering matrix
C = diag(35) - matrix(1/35,35,35)

# Here we'll center
Chills = hills
Chills[,2] = C%*%hills[,2]
Chills[,3] = C%*%hills[,3]

# And call lm
Cfit = lm(Time~Distance+Climb,data=Chills)
summary(Cfit)
```

2. Check that the intercept is, indeed, changed by the sum of the slopes times the average of each of distance and time: $\beta_1 \bar{x}_1 + \beta_2 \bar{x}_2$.

```{r,echo=TRUE}
fit$coef[1] - Cfit$coef[1] 

Cfit$coef[2]*mean(hills$Distance) + Cfit$coef[3]*mean(hills$Climb)
```


3. Confirm that $X^TX$ is block diagonal when the centered covariates are used. 
```{r,echo=TRUE}
X = as.matrix(cbind(rep(1,35),Chills[,2:3]))
t(X)%*%X

# Here we note that the off-diagonals on the first row and column are very close to zero. 
```


4. Now we will re-create the functions in 
```{r,echo=TRUE}
anova(fit)
```
   a. First, create the matrices $X_1 = [\mathbf 1_n, \mathbf x_1]$, and the matrix $X_2 = [X_1, \mathbf x_2]$ along with the corresponding hat matrices $H_1$ and $H_2$. 
```{r,echo=TRUE}
X1 = as.matrix(cbind(rep(1,35),hills[,2]))
X2 = as.matrix(cbind(X1,hills[,3]))

H1 = X1%*%solve(t(X1)%*%X1)%*%t(X1)
H2 = X2%*%solve(t(X2)%*%X2)%*%t(X2)
```

   b.  Form the matrices for the sums of squares $A_1 = H_1 C H_1$,  $A_2 = H - H_1$, $A_3 = I-H$, what are their traces?
```{r,echo=TRUE}
A1 = H1%*%C%*%H1
sum(diag(A1))

A2 = H2 - H1 
sum(diag(A2))

A3 = diag(35) - H2
sum(diag(A3))
```


   c. Show that $\mathbf y^T A \mathbf y$ gives the sum of squares for the corresponding term in _anova(fit)_ above. 
```{r,echo=TRUE}
y = hills$Time

y%*%A1%*%y
y%*%A2%*%y
y%*%A3%*%y


```
   d. Confirm that these sum to $\mathbf y^T c \mathbf y$. 
```{r,echo=TRUE}
y%*%C%*%y

y%*%A1%*%y + y%*%A2%*%y + y%*%A3%*%y
```
   
   
   e. Verify that these are also respectively the sum of squared differences between  
      i) a regression on Distance and the mean
      ii) a regression on both Distance and Climb and a regression on distance
      iii) the observations and a regression on both variables
      Use the hat matrices that you created to do this. 

```{r,echo=TRUE}
# First A1
y%*%A1%*%y

yhat1 = H1%*%y
sum( (yhat1 - mean(y))^2 )


# Now A2
y%*%A2%*%y

yhat2 = H2%*%y
sum( (yhat2 - yhat1)^2 )


# And A3
y%*%A3%*%y
sum( (y - yhat2)^2 )
```