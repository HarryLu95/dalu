---
title: "Lab 2 - Matrix Algebra and Linear Regression in R"
output:
  pdf_document: default
  html_document:
    highlight: pygments
    theme: cerulean
  word_document: default
---
-----

# Lab Goals

This lab is intended to 

1. Review matrix operations in R. In particular, we will examine
  
  a. functions to define matrices in R
  
  b. matrix operations and in particular the difference between element-wise and matrix multiplication.
   
  c. matrix transposes and inverses
  
  d. eigen analysis of a matrix
  
2. Review linear regression in R, and reconstructing the elements of `lm()` from scratch.    

3. Conduct a brief simulation to verify some theoretical results.

# Matrix Algebra in R

## Defining vectors 

We start by constructing a vector (a 1-dimensional array) by simply imputting a set of numbers using the *concatination* function `c()`:
```{r,echo=TRUE}
x=c(1,2,3)
x
```
This is not always very conventient, so we can also some shortcuts. We can get the numbers from 1 to 3 by
```{r,echo=TRUE}
x = 1:3
x
```
This is good for runs of integers, but we might also want other sequences, for this we can use
```{r,echo=TRUE}
x = seq(from=1,to=3,by=1)
x
```
*Important Note*:
The function `seq()` has *named* arguments `from` (the starting point of the sequence), `to` (the ending point) and `by` (the size of the steps to take).  These are read in this order by default so that `seq(1,3,1)` gives the same answer as above. 
  However, you can also change the order, `seq(by=1,from=1,to=3)` also gives the same output, but `seq(3,1,1)` produces an error (try it!). 
  While R programmers (the course staff included) often skip the argument names, *it is good coding practice to always put them in, so you know what R is treating as which argument*; we will try to stick to this in BTRY/STCI 4030. 

*Question*:
  1. How would you use this to produce a vector (1,1.5,2,2.5,3,3.5,4)? What about (4,3,2,1)?
  
  2. What if I wanted to produce 15 numbers between 1 and 4 without calculating the step size? (Hint, look at the help function for other arguments. 
  
We might also want to look at repeated numbers, for this try 
```{r,echo=TRUE}
rep(x=1,times=3)
```
Notice that you can also repeat a vector:
```{r,echo=TRUE}
rep(x=x,times=22)
```
*Question*: what if we wanted to repeat each element of `x` twice to get (1,1,2,2,3,3)?

## Defining matrices (2 dimensional arrays)

The `matrix()` command is the starting place here. 
```{r}
A=matrix(data=1:6,nrow=2,ncol=3)
```
Notice that this fills the 2-by-3 matrix going down rows, we might want to go the other way
```{r}
B=matrix(data=1:6,byrow=T,ncol=3)
```
Here R has worked out that it needs 2 rows if `data` is of length 6 and there are 3 columns. *It is nonetheless good practice to specify all dimensions, so that you get a warning if something isn't how you expect it to be.*

We can also combine vectors by row or by column:
```{r,echo=TRUE}
C=cbind(1:3,4:6)                         # column bind
D=rbind(1:2,3:4,5:6)                     # row bind
```
This also works if we want to stack matrices -- try row binding `A` and `B`. 

*Question*: What happens if you try to row bind `A` and `C`?

## Matrix operations

### Addition and Subtraction
Addition and subtraction are defined element-wise for matrices. We just add up the matching elements
```{r,echo=TRUE}
A+B
A-B
```
But note that both matrices have to have the same dimension
```{r,echo=TRUE,error=TRUE}
A+C                                      # non-conformable
```

*Question*: Give two ways that you might try to add $A$ and $C$. 

### Transpose
The function `t()` takes the transpose of a matrix:
```{r,echo=TRUE}
t(B)
t(A)
```

### Multiplication
Matrix multiplication is a little more tricky -- there are two types of matrix product. 

The `*` operator takes an *element-wise* product -- multiplying the corresponding elements together:
```{r,echo=TRUE}
A*B                                      # element-wise product
```
(like addition, `A` and `B` have to have the same dimensions. You can't do `A*C` -- try it). 

You can produce standard matrix muliplication using
```{r,echo=TRUE}
A%*%C                         
```
so long as the inner dimensions match.  Not that in this case
```{r,echo=TRUE,error=TRUE}
A%*%B                                    # non-conformable
```
does not work. 
 
In BTRY/STSCI 4030, we will be particularly interested in a matrix operating on a vector. 
We can achieve this by defining our vector as an n-by-1 matrix 
```{r}
y=matrix(1:3,3,1)
A%*%y
```
But R will also interpret a vector in exactly this way
```{r}
A%*%x                                    # x is treated the same as a 3 by 1 matrix
```
Note that R does keep different types of object in memory, so it believes that `y` is a matrix but that `x` is not:
```{r}
is.matrix(y)
is.matrix(x)
```
Nonetheless, we can perform element-wise calculations
```{r}
x*y
```
As well as obtaining the inner product (just a number)
```{r}
t(x)%*%y                                 # inner product of two vectors
```
And the outer product -- a matrix of ever possible combination of element products)
```{r}
y%*%t(x)                                 # outer product of two vectors
```

*Question* 

Matrix multiplication and addition follow the usual order of operations and in particular, $(A+B)C = AC + BC$

1. Show that this is the case for our $A$, $B$ and $C$. 

2. In general, by looking at $[(A+B)C]_{ij}$, show that this always holds (pen and paper exercize). 

### Matrix inverses

We can only invert a square matrix, so let's produce one, say:
```{r}
M = matrix(data=c(1,1,-1,0),nrow=2,ncol=2)
```
We can now solve this matrix with
```{r,echo=TRUE}
iM = solve(M)
```
and we can verify that this is indeed the inverse
```{r}
M%*%iM
```
*Exercize* verify that this is a left inverse as well as a right inverse. Why should they be the same? Verify that `M` is the inverse of `iM`. 

Let's try this on a larger, more complicated matrix, say with random entries:
```{r}
M=matrix(data=rnorm(9),nrow=3,ncol=3)
iM=solve(M)
iM%*%M
```
What has happened here? You'll notice that all the entries of the product are *close to* what they should be and if we `round` them we get something sensible
```{r}
?round                                  # get help on "round" function
help(round)
round(iM%*%M,1)
```
Here we get imperfect answers because when R calculates `iM`, the operations all drop some decimal places(it only stores numbers to about 16 places -- see BTRY 3520), which corresponds to an accuracy of about 1e-15, which is what we see. 


### Spectral Decompositions

Generally, we will only need the spectral decomposition as a theoretical construct, but it's worth looking at briefly. The `eigen` function will do this for you. Here we will use a 3 by 3 matrix
```{r}
D = diag(3) + matrix(1,nrow=3,ncol=3)
E = eigen(D)
```
Where the `values` element of the resulting list gives the eigenvalues and the `vectors` are the eigen-vectors. From this, we should be able to reconstruct $D$ by: 
```{r}
E$vectors %*% diag(E$values) %*% t(E$vectors)
```

*Questions:*
1. We saw in class that we can calculate $D^2$ by squaring the eigenvalues. Show that this is the case here, and that the eigenvalues of $D^2$ are the squares of `E$values`. 

2. Veryify that $tr(D)$ is the sum of its eigenvalues. 



# Linear Regression

Here we will use a new data set. We did this manually before, but you can also write code to read the data in automatically:
```{r}
hills=read.csv("hills.csv")
names(hills)
```

We can take a look at what these data look like
```{r} 
plot(hills$Distance,hills$Time)
```
and fit a linear model
```{r}
fit=lm(Time~Distance,data=hills)
summary(fit)
```
We can look at a sum of squares for this fit
```{r}
anova(fit)
```
and extract the coefficients and add them to a plot
```{r,echo=TRUE}
plot(hills$Distance,hills$Time)
b=fit$coef
abline(b)
abline(b,lwd=2,col="red")
```

# Reconstrucing Linear Regression

The purpose of BTRY/STSCI 4030 is to recover the mathematics that gets you these estimates, so let's reconstruct this.

First, to get things into 'standard' notation, we'll re-name some things:
```{r,echo=TRUE}
y=hills$Time
n=length(y)
```
We also need an $X$ matrix, which includes the column of 1's along with Distance,
```{r}
X=cbind(rep(1,n),hills$Distance)
```
Now, we can build up the estimate $\hat{\beta} = (X^T X)^{-1} X^T y$ in steps
```{r,echo=TRUE}
XtX=t(X)%*%X         
iXtX=solve(XtX)
bhat=iXtX%*%t(X)%*%y                      
```
*Exercize*: compare this with the estimates from `lm()`: how would you measure the discrepancy between these in R?

In order to get a confidence intervals for the slope, we need to obtain the residual 
standard error. For this, we first need to calculate the residuals $r = y - X \beta$:
```{r,echo=TRUE}
r=y - X%*%bhat                          
```
These are then squared and divided by the residual degrees of freedom to get the variance; we can get the standard deviation by obtaining their standard error
```{r,echo=TRUE}
s2hat=sum(r^2)/(n-2)
shat=sqrt(s2hat)                          
```
Compare this with "Residual standard error" from the lm summary output. 

Now we need to obtain the variance of $\hat{\beta}$, which we know is $\sigma^2 (X^T X)^{-1}$. In this case, we'll just extract the diagonals (ie, get the variance of $\beta_0$ and $\beta_1$ rather than worry about their covariance):
```{r,echo=TRUE}
se=sqrt(s2hat*diag(iXtX))                
```
which you should compare to the standard error values from the lm summary. We'll use the standard error of the slope below, so it will be helpful to extract it
```{r}
se2 = se[2]                              
```


# A Simulation

We have nice theory that says that the estimate $\hat{\beta}_1$ ought to be approximately normally distributed with the standard deviation that we calculated above. Let's see if it's true. 

To do this, we'll conduct a simulation. The interpretation of the statement above is that "If we repeated the experiment many times, the collection of estimates ought to have a normal-ish histogram". So let's try it.  

Each of 1000 times, we'll create a new "data set",by taking the same slope, intercept and $x$ values, but we'll generate our own standard errors. We can then "re-estimate" $\hat{\beta}$.  We'll record the value each time we do it, and then we can create a histogram of these values. 

First we need to do a bit of preparation, creating matrices to record the y's and the coefficients:
```{r,echo=TRUE}
N=1000            # This many simulations
Y=matrix(0,N,n)   # One row of y's for each simulation
B=matrix(0,N,2)   # One row of coefficient estimates for each simulation
```
Now we run through the simulations using a `for` loop. The syntax below says the following: 
   1. Set `i` to be each of the elements of `1:N` in turn. 
   2. Run the code in the braces for that value of `i`
In this case, for each of `i` is 1 up to 1000, we simulate new data, estimate parameters and record these in the `i`th row of `B`:
```{r,echo=FALSE}
for (i in 1:N)
  {
    y=b[1]+b[2]*hills$Distance+rnorm(n,sd=shat) # new simulated data
    Y[i,]=y                               # record these responses
    B[i,]=lm(y~hills$Distance)$coef             # record parameter estimates from new `data`
  }
```
Just to check that `Y` and `B` are of the right size:
```{r,echo=TRUE}
dim(Y); dim(B)
```
Now let's have a look at the histogram of simulated slopes
```{r,echo=TRUE}
hist(B[,2])
```
We can change the option, so we get a probability rather than a frequency:
```{r,echo=TRUE}
hist(B[,2],prob=TRUE)
```
This should be approximately normal with the correct mean and varaince, so let's 
overlay the density that we hope to see. 

First we define a set of plotting points, then we evaluate the density using `dnorm`:
```{r,echo=FALSE}
x=b[2]-3*se2+6*se2*(0:100)/100 # sequence of x values range over 6 se's
f=dnorm(x,mean=b[2],sd=se2)
fmax=max(f)
```
Now we re-draw the histogram and overlay the density
```{r,echo=TRUE}
hist(B[,2],prob=TRUE,ylim=c(0,1.1*fmax),
     main="Sampling Distribution of the Slope in SLR",
     xlab="Simulated Slope")
lines(x,f,col="red")
```
This looks pretty good. 

An alternative way of evaluating normality in our collection of estimated slopes is to produce a QQ-plot:
```{r}
qqplot(qnorm((1:N)/(N+1)),B[,2])
qqline(B[,2],col="red")
```


