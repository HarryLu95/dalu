---
title: "Lab 3 - On the Effect of Centering Variables"
output:
  pdf_document: default
  html_document:
    highlight: pygments
    theme: cerulean
  word_document: default
---
-----

# Lab Goals

The first two questions in this lab are designed to make you practice some theoretical calculations, while the third confirms these results using the \texttt{hills} data from Lab 2.  When going through these, you may find it helps to have a very simple example to work through, either by hand or in R. Here we recommend using a $p =2$ case with
$$
\boldmath{x}_1 = (1,2,2,3), \ \boldmath{x}_2 = (1,1,2,4), \ \beta = (1,1,1), \ \boldmath{y} = (2,4,6,8)
$$

We'll set this up in R:
```{r}
x1 = c(1,2,2,3)
x2 = c(1,1,2,4)
beta = c(1,1,1)
y = c(2,4,6,8)
```
and we'll also define $X$ and $C$ matrices
```{r}
X = cbind( rep(1,4), x1, x2)
C = diag(4) - matrix(1/4,4,4)
```
which we will use later.  It will also help to define centered versions of covariates:
```{r}
x1c = C%*%x1
x2c = C%*%x2
Xc = cbind( rep(1,4), x1c, x2c)
```

# Centering, Linear Regression and ANOVAs

In class, we've relied on centered variables to clean up some calculations, and we've (rather blythely) said that this is OK (see the derivation of VIFs for example). Here we'll work through showing that this is the case:

1. By manipulating the equation
$$
\boldmath{y} = \beta_0 + \beta_1 \boldmath{x}_1 + \ldots + \beta_p \boldmath{x}_p + \epsilon
$$
show that changing covariates from $\boldmath{x}_j$ to $\boldmath{x}^*_j = C\boldmath{x}_j$ changes $\beta_0$ but not $\beta_j$.




2. Show that the *estimated* $\hat{\beta}_j$ are also unaffected by centering the covariates. Do this by arguing that the *fitted* *values* are the same and these minimize mean squared error. Below, we will show that this is true by explicitly using matrix algebra. 




In R, we see that we can run either
```{r,echo=TRUE}
mod = lm(y~x1+x2)
mod$coef
```
or
```{r,echo=TRUE}
modc = lm(y~x1c+x2c)
modc$coef
```

3. In simple linear regression, when $\boldmath{x}$ is centered, there is a simple formula for $\hat{\beta}_0$ and $\hat{\beta}_1$ and these are uncorrelated.  When $\boldmath{x}$ is *not* centered, use the formula that for a $2 \times 2$ matrix,
$$
    \left[ \begin{array}{cc} a & b \\ c & d \end{array} \right]^{-1} = \frac{1}{ad-cb} \left[ \begin{array}{cc} d & -c \\ -b & a \end{array} \right]
$$
    to find the covariance of $(\hat{\beta}_0,\hat{\beta}_1)$ and show that centering reduces the variance of $\hat{\beta}_0$ but leaves the variance of $\hat{\beta}_1$ unchanged. \label{invariant}

In our toy example in R, we can consider only the first covariate and look at
```{r,echo=TRUE}
mod1 = lm(y~x1)
vcov(mod1)
```
versus
```{r,echo=TRUE}
mod1c = lm(y~x1c)
vcov(mod1c)
```

4. If $X$ is the design matrix, including the intercept, for the model above, find $A$ so that $XA = X^*$ where the columns of $X^*$ are all centered except for the first column of 1's.

As an example in R, if we consider
```{r}
A = matrix( c(1,-2,-2,0,1,0,0,0,1),3,3,byrow=TRUE)
X%*%A
```
compare to 
```{r}
Xc
```

5. Find $A^{-1}$ from the previous question (hint: $A$ will be given in terms of the $\bar{x}_j$, replace each of these with $- \bar{x}_j$ and show that this gives you the inverse).
```{r}
solve(A)
```

5. By expanding $(X^{*T} X^*)^{-1}$ in terms of $(X^T X)^{-1}$ and $A^{-1}$, show that the result from \eqref{invariant} is also true of multiple regression. Hence our Variance Inflation Factor identity also holds for un-centered covariates.

```{r}
vcov(mod)
vcov(modc)
```


# In Data

Let's look at what we actually get in data. For this question, we use the data used in Lab 2.


```{r}
hills=read.csv("hills.csv")
fit=lm(Time~Distance+Climb,data=hills)
summary(fit)
```

1. Create a centering matrix and apply this to center Distance and Climb. Show that applying lm to these data gives you the same slopes, but changes the intercept. 

2. Check that the intercept is, indeed, changed by the sum of the slopes times the average of each of distance and time: $\beta_1 \bar{x}_1 + \beta_2 \bar{x}_2$.

3. Confirm that $X^TX$ is block diagonal when the centered covariates are used. 

4. Now we will re-create the functions in 
```{r,echo=TRUE}
anova(fit)
```
   a. First, create the matrices $X_1 = [\mathbf 1_n, \mathbf x_1]$, and the matrix $X = [X_1, \mathbf x_2]$ along with the corresponding hat matrices $H_1$ and $H_2$. 

   b.  Form the matrices for the sums of squares $H_1 C H_1$,  $H - H_1$, $I-H$, what are their traces?

   c. Show that $\mathbf y^T A \mathbf y$ gives the sum of squares for the corresponding term in _anova(fit)_ above. 

   d. Confirm that these sum to $\mathbf y^T c \mathbf y$. 

   e. Verify that these are also respectively the sum of squared differences between  
      i) a regression on Distance and the mean
      ii) a regression on both Distance and Climb and a regression on distance
      iii) the observations and a regression on both variables
      Use the hat matrices that you created to do this. 
